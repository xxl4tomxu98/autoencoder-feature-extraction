{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA uses linear transfomation to reduce the dimensions of our dataset. And as a bonus, because PCA transforms the data by exploring ‘linear’ covariances between the variables it can also be used as an anomaly detector. Because any recipe that doesn’t follow the ‘structure’ of the initial dataset, won’t transform well.\n",
    "\n",
    "6714 ingredients -> 6714 columns. When one ingredient is present in a recipe, its column goes to 1. All the rest stays as a 0. In average only 10 of those columns will be ‘active’ in each row. This code will create the “transformer”, that will get an ingredient and output its vector representation. Following code gives us an encoder that will get a ingredient (string) as input and output its vector representation. The final vector containing all the recipe’s ingredients will be the result of a ‘logical or’ on every one of those ingredient vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import array\n",
    "import json\n",
    "f = open('train.json', 'r')\n",
    "recipes_train_txt = f.read()\n",
    "recipes_train_json = json.loads(recipes_train_txt)\n",
    "#get list of ingredients\n",
    "ingredients = set()\n",
    "ingredients_matrix = []\n",
    "for recipe in recipes_train_json:\n",
    "    ingredients_matrix.append(recipe[\"ingredients\"])\n",
    "    for ingred in recipe[\"ingredients\"]:\n",
    "        ingredients.add(ingred)\n",
    "ingredients = list(ingredients)\n",
    "ingredients.sort() #it made my life easier to have it sorted when i needed to check what is what in the encoded vector\n",
    "values = array(ingredients) \n",
    "label_encoder = LabelEncoder()\n",
    "#gives a unique int value for each string ingredient, and saves the #mapping. you need that for the encoder. something like: \n",
    "#['banana'] -> [1]\n",
    "integer_encoded = label_encoder.fit_transform(values) \n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "#here you encode something like : [2] -> [0,1,0,0,...]\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "def transform_value(s):\n",
    "    \n",
    "    l = array([s])\n",
    "    integer_encoded = label_encoder.transform(l)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.transform(integer_encoded)\n",
    "    \n",
    "    return onehot_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = len(X_Total[0])\n",
    "num_hidden_l = 700\n",
    "X = tf.placeholder(“float”, [None, num_input])\n",
    "w_encoder_h1 =  tf.Variable(tf.random_normal([num_input, num_hidden_l])\n",
    "w_decoder_h2 = tf.Variable(tf.random_normal([num_hidden_l, num_input]))\n",
    "encoder_b1 = tf.Variable(tf.random_normal([num_hidden_l]))\n",
    "decoder_b2 = tf.Variable(tf.random_normal([num_input]))\n",
    "layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, w_encoder_h1),\n",
    "                                   encoder_b1))\n",
    "layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w_decoder_h2),\n",
    "                                  decoder_b2))\n",
    "# Prediction\n",
    "y_pred = layer_2\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79f4630616981068147ecb693f55d51ab12fab43ffc02db62e4992b7ed83fc2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf2.5': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
